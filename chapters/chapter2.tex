\chapter{3D Model Building}
This chapter contains a short overview on how to build a generative 3D face model from 3D face scans. (because this is what should eventually be achieved with this thesis). The term generative describes the fact that with a linear face model, arbitrary, feasible faces can be generated by combining and scaling faces that are part of the model.

derive morphable face model from an example set of 3D face models
\section{3D Morphable Model}
The 3D Morphable Model (3DMM) published by Blanz and Vetter in 1999 (bib) is such a generative 3D face model. It is a linear model built on a set of faces parametrized by cofficients $\vec \alpha$ and a set of textures parametrized by coefficients $\vec \beta$
\begin{comment}
It combines a linear model of faces $S(\alpha)$ with a linear model $T(\beta)$ of the corresponding textures.    
\end{comment}

\begin{equation}
    \mathcal{S}(\alpha)=s+S\alpha \quad \mathcal{T}(\beta)=t+T\beta 
\end{equation}
The average of the faces $s \in \mathbb{R}^{3N}$ and of the textures $t \in \mathbb{R}^{3N}$ are used to fit two multivariate normal distributions to the whole dataset of faces. The covariance matrices are defined over the differences between each face and the mean face, the same applies to the textures.
We get two distributions.
A set of faces parametrized by coefficients a, set of Textures parametrized by coefficients b

\begin{equation}

\end{equation}
Fit multivariate normal distribution to data set, based on average of faces and textures. Build covariance matrices over differences between the mean and face samples in surface and texture. => two distributions. Perform PCA to get orthogonal basis system
In the MM three subspaces are morphed independently

Eine Gruppe (G,◦) heisst abelsch [abelian] oder kommutativ wenn a◦b = b◦a gilt für
alle a,b ∈ G.

2.3 Building a Model from the registered data (short)
\section{Prerequisite Data}
image with landmarks and line features
a short overview what data we have given

Facial Scans:
face scans given as point clouds
The data we have given is a set of about 300 face scans that have had a set of key points marked. Furthermore important and detailed regions like the eyes, ears and lips have been marked by contour lines known as line features. The scans have been obtained with a … scanner. The surface is very detailed, however the eyes and the nostrils are not recorded. From these scans we want to create fully textured 3D faces, which can be used to build a new face model.

Mean Face:
The mean face has been derived from a collection of 100 male and 100 female 3D face models.


\section{Finding Correspondences}
WE WANT POINT TO POINT CORRESPONDENCE BETWEEN THE TWO FACES
in general: point to point correspondence between to images
Are scans already in semantical correspondence? No semantical correspondence
FINDING CORRESPONDENCE IS EXACTLY THE AIM OF REGISTRATION => HAVING SAME POINTS AS CLOSE TO ONE ANOTHER AS POSSIBLE
Now in order to obtain a 3D representations of the face we need to transform the mean face so that it fits a particular 3D face scan. To find the transformation, however, we first have to find feature points in both 3D representations which correspond to the same semantical structure. Previous work has shown that point landmarks are not sufficient to preserve the level of detail which is imminent in the regions of the eyes, ears and lips and that the computed transformations are not able to preserve these regions. For this reason, additional line features have been introduced. In order to relate these 

How registration works so far

What we want to change


