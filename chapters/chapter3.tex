\chapter{Gaussian Processes in 3D Face Registration}
The first of our two objectives is to build a face registration pipeline. 
In this context we use a stochastic process, more specifically a vector-valued Gaussian process or Gaussian random field as the registration algorithm.  
To begin with, we recapitulate the definition
of stochastic processes and extend it to the definition of Gaussian processes. In the next step we introduce Gaussian Process Regression and finally explain it can be applied 3D face mesh registration. 

\section{Stochastic Processes}
\begin{comment}
Assigns random variables to every point in time series or space 
\end{comment}

In probability theory a stochastic process consists of a collection of random variables $\{X(t)\}_{t \in \Omega}$ where $\Omega$ is an index set. It is used to model the change of a random value over time. The underlying parameter time is either real or integer valued. A generalization of a stochastic process, which can handle multidimensional vectors, is called a random field. 
\begin{comment}
A stochastic process is made up of a probability space (sample space, set of events, assignment of probabilities to events P: E → [0,1]) and a sigma-algebra (sample-space, €). It assigns a collection of random variables to an index set.
\end{comment}

\section{Gaussian Processes}

\begin{comment}
Definition taken from Stochastic Simulation p.306
\end{comment}

A Gaussian process is a stochastic process in which each finite collection $\Omega_{0} \subset \Omega$ of random variables has a joint normal distribution. More formally, we define the collection of random variables $\{X(t)\}_{t \in \Omega}$ to have a d-dimensional normal distribution if the collection $\{X(t)\}_{t \in \Omega_{0}}$ - for any finite subset $\Omega_{0}$ - has a joint $d\times \left| \Omega_{0} \right|$-dimensional normal distribution with mean
$\mu (\Omega_{0})$ and covariance $\Sigma (\Omega_{0})$.  
If $\Omega \subseteq \mathbb{R}^{n}, n>1$ holds, the process is a Gaussian random field. In the further proceedings the term ``Vector-valued Gaussian Processes'' will be used to refer to Gaussian random fields. Defining the random variables on an index set in an n-dimensional space, allows for spatial correlation of the resulting values, which is an important aspect of the algorithm discussed later on.

\begin{comment} move to 3.4!! In our case, however, we use a collection of vector-valued random variables with indices in the $\mathbb{R}^{3}$ space because we want to model deformations of the vectors on the faces' surfaces.
\end{comment}
\begin{comment}
Now incorporate definition on p. 13 of Gaussian processes for machine learning
\end{comment}

An alternative way of viewing a Gaussian process is to consider it as a distribution over functions. This allows us to look for inference in the space of these functions given a dataset, specifically to find the deformation function given a 3D face mesh. Each random variable now yields the value of a function $f(x)$ at a location $x \in \mathcal{X}$ in the index set of possible inputs. We now denote the index set by $\mathcal{X}$ to stress that we are ceasing to discuss
Gaussian processes defined over time. In this function-space view a Gaussian Process at location x is thus $f(x) \sim GP(\mu(x), k(x,x'))$ defined by its mean $\mu:\mathcal{X}
\rightarrow \mathbb{R}$ and covariance $k:\mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ functions which in turn are defined over the set of input vectors. With $\mu(\mathcal{X})=(\mu(x))_{x \in \mathcal{X}}$ and $\Sigma(\mathcal{X})=(k(x,x'))_{x,x' \in \mathcal{X}}$ we obtain the full distribution of the process $GP(\mu(\mathcal{X}), \Sigma(\mathcal{X}))$. For the purpose of simplifying calculations we may assume that every random variable has zero mean without a loss of generality. When modeling a deformation field with a Gaussian process this circumstance implies that the expected deformation is itself zero. 

\begin{comment}
Define Gaussian process for location x or for whole index set?
\end{comment}

\paragraph{Covariance Functions}
The key feature of a Gaussian Process is its covariance function also known as ``kernel''. It specifies the covariance $\mathbb{E}[f(x)f(x')]$ between pairs of random variables for two input vectors $x$ and $x'$, allowing us to make assumptions about the input space by defining the spatial co-dependency of the modelled random variables. Note that when assuming zero mean we can completely define the process' behaviour with the covariance function.\\
A simple example of a covariance function is the squared exponential covariance function, defined by $cov(f(x),f(x'))=k(x,x')=\exp(-\frac{(x-x')}{2l^{2}})$. (derivation Rasmussen et al. p.83) \textit{still to be continued and refined\ldots}  

\begin{comment}
% future work: tweak parameters, try out other covariance functions
\end{comment}

It is possible to obtain different prior models by using different covariance functions. In our case, we use a stationary (x-x’, invariant to translation), isotropic exponential covariance function - Squared Exponential Covariance Function (p. 38)

\paragraph{Gaussian Process Prior}
The specification of the covariance function implies that a GP is a distribution over functions. To illustrate this one can draw samples from a prior distribution of functions evaluated at any number of points, $X_{*}$. The Gaussian Process Prior is solely defined by the covariance matrix made up of the covariances of the input points.

\begin{equation}
    \Sigma(X_{*})=\begin{bmatrix}
k(x_{*1},x_{*2}) & \cdots & cov(f(x_{*1},f(x_{*n}) \\
\vdots & \ddots & \vdots \\
cov(f(x_{*n},f(x_{*1}) & \cdots & cov(f(x_{*n},f(x_{*n}) 
\end{bmatrix} \in \mathcal{M}^{\left|X_{*}\right| \times \left|X_{*}\right|} 
\end{equation}

A sample is a random Gaussian vector $f_{*} \sim \mathcal{N}(0, \Sigma(X_{*}))$ containing a function value for every given input point. Plotting random
samples above their input points is a nice way of illustrating that a GP is indead a distribution over functions, see figure \ref{fig:GPPlot}. The GP Prior forms the basis for inference in Gaussian Process Regression.

\begin{figure}[h!]
    \subfloat[Samples drawn from the prior]{\includegraphics[width=.5\textwidth]{./resources/figures/gp_prior.eps}}
    \subfloat[Samples after a few observations]{\includegraphics[width=.5\textwidth]{./resources/figures/gp_posterior.eps}}
\caption{In figure a) three functions have been drawn from the GP prior, each has a sample size of 1000 points. Figure b) again shows three functions, but this time the prediction incorporates the information of random values observed at seven points in the input space}
\label{fig:GPPlot}
% reference in text by \ref{$figure-name}
\end{figure}

\paragraph{Vector-valued Gaussian Processes}
In order to use Gaussian processes to model deformation fields of three dimensional vectors as intended, there is the need for a generalization of the above definition from the function-space view. The random variables $X_{1}, X_{2}, \ldots, X_{k}, \ldots, X_{n}$ are now d-dimensional vectors, yielding a covariance function of the form $k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}^{d \times d}$ and $k(x,x')=\mathbb{E}[X_{k}(x)^{T}X_{k}(x')]$. \textit{Should this paragraph be continued?}

\section{Gaussian Process Regression}
The task of registering two 3D face meshs can be treated as a regression problem in which the goal is to predict the deformation of all floating mesh points, given the displacement of the landmarks present in both meshs. Trying to fit an expected function - be it linear, quadratic, cubic or nonpolynomial - to the data is not a sufficiently elaborated approach to our problem. 
Using a Gaussian Process disposes of the need to describe the data by a specific function type, because the response for every input point is now represented by a normally distributed random value, in turn governed by the specification of the covariance function.

Key assumption: data can be represented as a sample from a multivariate gaussian distribution
P

\paragraph{Regression Problem}
Assume a training set $\mathcal{D} = \{(x_{1},y_{1}), (x_{2},y_{2}), \cdots, (x_{n},y_{n})\}$ where $x \in \mathbb{R}^{d}$ and y is a scalar output or target. 
\begin{comment}Later on, in the case of the training set consisting of landmarks, a Vector-valued Gaussian Process must be used, because y is then also a vector $y \in \mathbb{R}^{d}$.\end{comment} 
The task is now to infer the conditional distribution of the targets for yet unseen inputs and given the training data $p(\textbf{f}_{*}\vert \textbf{x}_{*},
\mathcal{D})$

\paragraph{Noise-free Prediction}
First we assume the observations from the training data to be noise-free so that we can fix the training data to these observations $\textbf{y}$ without complicating the model. The joint prior distribution with training $\textbf{f}$ and test $\textbf{f}_{*}$ outputs indicated is the following:

\begin{comment}
For every point we test, we extend the joint distribution by an input points, adding covariances and extending the dimensionality of the distribution
\end{comment}

\begin{equation}
\begin{bmatrix}\textbf{f}\\\textbf{f}_{*}\end{bmatrix}
\sim \mathcal{N}\left(\textbf{0},
\begin{bmatrix}
    \Sigma(X) & \Sigma(X,X_{*})\\
    \Sigma(X_{*},X) & \Sigma(X_{*})\\
\end{bmatrix}
\right)
\end{equation}

We obtain the posterior samples illustrated in \ref{fig:GPPlot} b) by conditioning the above joint Gaussian prior distribution on the observations $\textbf{f}_{*}\vert\textbf{f}=\textbf{y}$ which results in the following distribution:

\begin{comment}
reference equations for the conditional distributions
for example rasmussen: A.2 Gaussian Identities, p.200
\end{comment}

\begin{equation}
    \textbf{f}_{*}\vert X_{*},(X,\textbf{f}) \sim \mathcal{N}\left(\Sigma(X_{*},X)\Sigma(X)^{-1}\textbf{f},\\\Sigma(X_{*})-\Sigma(X_{*},X)\Sigma(X)^{-1}\Sigma(X,X_{*})\right)
\end{equation}

\begin{comment}Later on, we will extend this definition to 3-dimensional inputs and outputs.\end{comment}

\paragraph{Prediction with Gaussian Noise Model}
In most real world applications\begin{comment}as is the case for the problem we will look into later\end{comment}, however, observations from the training data are not free of noise. The landmarks clicked on the 3D face meshs, for example, can never be marked at the exact same feature location. These circumstances call for the incorporation of a noise model. We specify a simple additive i.i.d Gaussian noise model $y = f(x) + \varepsilon$ where $\varepsilon \sim \mathcal{N}(0, \sigma^2)$ for every input vector x. 

\begin{comment}In section \ref{} the variances will be varied for every sole landmark. For now it is enough to add the variance of the noise model to the covariance of the training.\end{comment}

\begin{equation}
\begin{bmatrix}\textbf{y}\\\textbf{y}_{*}\end{bmatrix}
\sim \mathcal{N}\left(\textbf{0},
\begin{bmatrix}
    \Sigma(X) + \sigma^2\mathcal{I}_{\left|X \right|} & \Sigma(X,X_{*})\\
    \Sigma(X_{*},X) & \Sigma(X_{*})\\
\end{bmatrix}
\right)
\end{equation}

The distribution - now conditioned on the noisy observations - is thus
\begin{subequations}
\begin{equation}
    \textbf{y}_{*}\vert \textbf{f}=\textbf{y} \sim \mathcal{N}\left(\overline{\textbf{y}}_{*} ,\Sigma(\textbf{y}_{*})\right)
\label{eq:3.5a}
\end{equation}
where the mean depends on the observed training targets 
\begin{equation}
    \overline{\textbf{y}}_{*} = \Sigma(X_{*},X)\left(\Sigma(X)+\sigma^2\mathcal{I}_{\left|X \right|}\right)^{-1}\textbf{y}
\end{equation}
whilst the covariance depends only on the input points
\begin{equation}
    \Sigma_{*} = \Sigma(X_{*}) - \Sigma(X_{*},X)\left(\Sigma(X)+\sigma^2\mathcal{I}_{\left|X \right|}\right)^{-1}\Sigma(X,X_{*})
\end{equation}
\end{subequations}
\viscomment{Conclusion, how does this help us to proceed?}
  
\section{Application to 3D Face Meshs} 
In this section of we adapt the above presented theory to our case of 3D face mesh registration. The task at hand is to register a reference or \textbb{template} face mesh with a scanned face mesh. \viscomment{registration and correspondence already explained in model building, deformation field bold instead of calligraphic?}
We therefore strive to predict a deformation field $\mathcal{D}:\mathcal{M} \subset \mathbb{R}^3 \rightarrow \mathbb{R}^3$ which assigns a displacement vector to every vertex in the template mesh. During registration we refer to the template as the moving mesh $\mathcal{M}$. Adding the displacement field to the moving mesh should then provide an accurate mapping to the target mesh $\mathcal{T}$ and thereby perform the registration. Our objective is to register the template with
multiple meshs of scanned faces. 
\viscomment{Andreas: don't refer to 3DMM mean, because we haven't built a model yet! Leave out ``triangulated'', kind of mesh topology is not important in this thesis} 

\paragraph{Reference Mesh Prior}
As defined by the deformation field the output the regression problem is in $\mathbb{R}^3$ calling for the use of a Vector-valued Gaussian Process with random variables $d \subseteq \mathbb{R}^3$ where d stands for deformation. 
After the template and target have been aligned \ref{chapter 4} a Vector-valued Gaussian Process can be initialized by defining the prior over all vertices of the template mesh. For this purpose the covariance function has to be redefined to handle 3-dimensional vectors.
\viscomment{Prior consists of smooth deformations of the mean face}
\begin{equation}
    k\left(
    \begin{bmatrix}x_{1}\\x_{2}\\x_{3}\end{bmatrix},
    \begin{bmatrix}x'_{2}\\x'_{2}\\x'_{3}\end{bmatrix}
    \right) = x y^T \in M^{3 \times 3}
\end{equation}
Each covariance entails 9 relationships between the different components of the vectors, yielding a $3 \times 3$ matrix. The covariance matrix then grows to become: 
\begin{equation}
    \Sigma_{\mathcal{X}} = 
\begin{bmatrix}
    k(\vect{x}_{1},\vect{x}_{1}) & \cdots & k(\vect{x}_{1},\vect{x}_{n}) \\
\vdots & \ddots & \vdots \\
k(\vect{x}_{n},\vect{x}_{1}) & \cdots & k(\vect{x}_{n},\vect{x}_{n})
\end{bmatrix} \in M^{3n \times 3n}
\end{equation}

The template mesh is defined by a set of vectors $\mathcal{X} \in \mathbb{R}^3$ and a set of landmarks $L_\mathcal{M}=\{l_{1}, \cdots, l_{n}\} \subset \mathbb{R}^3$.
\viscomment{Introduce landmarks in model building}
The mean vector $\mu$ is made up of the component-wise listing of vectors so that it has dimensionality $3n$. Setting the whole mean vector to zero, as discussed before, implies a mean deformation of zero and makes perfect sense in this setting, because we are modelling deformations of the template surface. 
The prior distribution over the template mesh is therefore defined as 
\begin{equation}
    \mathcal{D} \sim \mathcal{N}(\vect{0}, \Sigma_{\mathcal{X}})
\end{equation}
meaning that a deformation field can be directly drawn as a sample from the prior distribution of the vertices of the template mesh. \viscomment{show two or three samples of prior here, next to template/mean mesh}

\paragraph{Reference Mesh Posterior}
The target landmarks also consist of a set $L_{\mathcal{T}} = \{l_{\mathcal{M}1},\cdots, l_{\mathcal{M}N}\} \subset \mathbb{R}^3$. 
Fixing the prior output to the deformation vectors $\mathcal{Y}=\{\vect{t}-\vect{m}\vert \vect{t} \in L_{\mathcal{T}}, \vect{m} \in L_{\mathcal{M}}\}$ defined by the distance between the template and target landmarks and assuming additive i.i.d Gaussian noise the resulting posterior distribution is
\begin{equation}
    \begin{bmatrix}\mathcal{Y_{\varepsilon}}\\\mathcal{D}\end{bmatrix}
\sim \mathcal{N}\left(\textbf{0},
\begin{bmatrix}
    \Sigma(\mathcal{Y}) + \sigma^2\mathcal{I}_{3\left|\mathcal{Y} \right|} & \Sigma(\mathcal{Y},X_{*})\\
    \Sigma(X_{*},\mathcal{Y}) & \Sigma(X_{*})\\
\end{bmatrix}
\right)
\end{equation}
\viscomment{Is this a correct definition for the distribution?}

The deformation model is now rendered fixed at certain landmark points in the target mesh and the goal is to find valid deformations through the set of fixed targets, analogous to the case of eq. \ref{eq:3.5a}. The posterior model is defined as the joint distribution of all template mesh points and the template landmarks, conditioned on the output deformation vectors for every template landmark with added noise.
\begin{center}
\begin{equation}
    \mathcal{D}\vert \mathcal{X} \rightarrow \mathcal{Y}_{\varepsilon}. 
\end{equation}
\end{center}
We now have defined a distribution over our template mesh. \viscomment{mean/template is now max aposteriori solution} Sampling the conditional distribution creates deformed 3D surfaces of the mean mesh which are fixed at the target landmarks.
\viscomment{show images of mean, prior and posterior with added landmarks}
\begin{comment}The variance of the gaussian kernel can thereby be described as a smoothing parameter P\end{comment}

\section{Fitting \& Optimization}
\label{optimization}

Due to the fact that the posterior model is fixed at the target landmarks it is possible to perform the registration by drawing a shape from the posterior model. The sample, however, has to be optimized according to the shape of the target face. 
In order to find the deformation corresponding to the optimal fit $d_{*}$ a linear optimization with the posterior process as a constraint is be employed/ regularization term. (small lambda) a bit of the posterior mean) 
\begin{equation}
d_{*} = \underset{d \in \mathcal{D}}{\textbf{arg min}}\quad L[O_{\mathcal{T}}, O_{\mathcal{M}} \circ d]+\lambda R[d]
\end{equation}
Minimizing a loss function L - mean square distance for example - on the target and the deformed mean provides a feasible deformation field. 
\mathcal{D} denotes the space of possible deformations in the posterior model. The information needed is held by the covariance matrix of the posterior process. However, using Mercer's theorem (in short what it does, yada yada yada) a basis of functions corresponding to all possible deformations can be extracted. The kernel is thus described as a linear model of these basis functions.    
\viscomment{whole matrix or simple kernel?}
\viscomment{refer to paper ``a unified formulation of statistical model fitting and non-rigid registration}
\begin{equation}
k(x,x') = \sum^{\infty}_{i=1}\lambda_{i}\phi_{i}(x)\phi_{i}(x')
\end{equation}
$\lambda_{i}$ are the eigenvalues and $\phi_{i}$ the eigenvectors of K. They denote the deformation directions while the eigenvalues \ldots
We are looking for a finite linear combination of eigenvectors that form a deformation field with $\exists \alpha_{1} \cdots \alpha_{n} \in \mathbb{R}$ as linear parameters.
\begin{equation}
f(x) = \sum^{n}_{i=1} \alpha_{i}\lambda_{i}\phi_{i}(x)
\end{equation}

f ~ GP(0, K) we take our gaussian process
> f | x=y, 
\viscomment{ask Marcel for a helping hand with the theory?}

Next, we want to minimize residuals for the whole of our surface according to optimal parameter values $\alpha_{i}$ 
\begin{equation}
    \underset{\alpha \in \mathbb{R}^n}{\textbf{arg min}}\quad \Sigma_{x_{i} \in \mathcal{T}_{R}} (f(x_{i}) - \phi_{T}(x_{i}))^2
\end{equation}
where $f(x_{i})$ is the deformation function and $\varphi_{T}(x_{i})$ returns the nearest point on the target mesh. Yields the overall loss function $\Phi_{L}$ 
\begin{equation}
\Phi_{L}(f(x_{i})-\varphi_{T}(x_{i}))
\end{equation}

The eigen vectors - which are deformation vectors defining a deformation for every model vertex - of the covariance matrix define a basis space? 
Shape Modell => select best eigenvectors via PCA in order to simplify computation.

=> Vorstellen wie wenn mehrere Wellbleche durch die Target"\_"landmarks gelegt werden und dann mit bestimmten parametern alpha zwischen ihnen interpoliert wird
Alternative way to understand basis functions for gaussian process:
sample from the GP(0, K)
and then build a linear model from the functions, f(x) = sum(i, n) alpha(i) si(x)
Posterior Distribution of Landmarks
Defining the Gaussian Process
Posterior Distribution - Landmarks (Referenz deformieren 
From Gaussian Processes to Shape Models => by selected principal components of the covariance matrix


